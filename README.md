# Ongoing Project(Learning Phase) 



# Attention in Transformers - DL.AI

This folder is part of the larger **Neural Network - Andrej Karpathy** project and focuses on implementing various concepts related to attention mechanisms in transformers. Each file contains detailed code and explanations provided in Jupyter notebooks.

---

## Repository Structure


| File Name | Description |
|-----------|-------------|
| [Lesson_3-Self-Attention.ipynb]([Lesson_3-Self-Attention.ipynb](https://github.com/Mofazzal874/Neural-Network-Andrej-Karpathy/blob/main/Attention%20in%20Transformer%20-%20DL.AI/Lesson_3-Self-Attention.ipynb)) | Implementation of **Self-Attention** mechanism, explaining how attention weights are computed and applied to input sequences. |
| [Lesson_6-Masked_Self-Attention.ipynb]([Lesson_6-Masked_Self-Attention.ipynb](https://github.com/Mofazzal874/Neural-Network-Andrej-Karpathy/blob/main/Attention%20in%20Transformer%20-%20DL.AI/Lesson_6-Masked_Self-Attention.ipynb)) | Implementation of **Masked Self-Attention**, used in tasks like autoregressive modeling where future tokens are masked during computation. |
| [Lesson_9-Attention(Self_Masked_Encoder-Decoder_Multi-Head).ipynb]([Lesson_9-Attention%28Self_Masked_Encoder-Decoder_Multi-Head%29.ipynb](https://github.com/Mofazzal874/Neural-Network-Andrej-Karpathy/blob/main/Attention%20in%20Transformer%20-%20DL.AI/Lesson_9-Attention(%20Self_Masked_Encoder-Decoder_Multi-Head).ipynb)) | Comprehensive implementation covering **Self-Attention**, **Masked Attention**, **Encoder-Decoder Attention**, and **Multi-Head Attention** mechanisms. 

---
